{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from itertools import cycle\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "DitU1eD6K8Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsZIvCOcMMdz",
        "outputId": "5b82f122-0858-494c-b1f0-b6e91491f640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "nIDuis3FLAk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files(folder, name_filter=None, extension_filter=None):\n",
        "    \"\"\"Helper function that returns the list of files in a specified folder\n",
        "    with a specified extension.\n",
        "\n",
        "    Keyword arguments:\n",
        "    - folder (``string``): The path to a folder.\n",
        "    - name_filter (```string``, optional): The returned files must contain\n",
        "    this substring in their filename. Default: None; files are not filtered.\n",
        "    - extension_filter (``string``, optional): The desired file extension.\n",
        "    Default: None; files are not filtered\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(folder):\n",
        "        raise RuntimeError(\"\\\"{0}\\\" is not a folder.\".format(folder))\n",
        "\n",
        "    # Filename filter: if not specified don't filter (condition always true);\n",
        "    # otherwise, use a lambda expression to filter out files that do not\n",
        "    # contain \"name_filter\"\n",
        "    if name_filter is None:\n",
        "        # This looks hackish...there is probably a better way\n",
        "        name_cond = lambda filename: True\n",
        "    else:\n",
        "        name_cond = lambda filename: name_filter in filename\n",
        "\n",
        "    # Extension filter: if not specified don't filter (condition always true);\n",
        "    # otherwise, use a lambda expression to filter out files whose extension\n",
        "    # is not \"extension_filter\"\n",
        "    if extension_filter is None:\n",
        "        # This looks hackish...there is probably a better way\n",
        "        ext_cond = lambda filename: True\n",
        "    else:\n",
        "        ext_cond = lambda filename: filename.endswith(extension_filter)\n",
        "\n",
        "    filtered_files = []\n",
        "\n",
        "    # Explore the directory tree to get files that contain \"name_filter\" and\n",
        "    # with extension \"extension_filter\"\n",
        "    for path, _, files in os.walk(folder):\n",
        "        files.sort()\n",
        "        for file in files:\n",
        "            if name_cond(file) and ext_cond(file):\n",
        "                #full_path = os.path.join(path, file)\n",
        "                filtered_files.append(file)\n",
        "\n",
        "    return filtered_files\n",
        "\n",
        "\n",
        "\n",
        "learning_map = {\n",
        "  0 : 0,    # \"unlabeled\"\n",
        "  1 : 0,     # \"outlier\" mapped to \"unlabeled\" --------------------------mapped\n",
        "  10: 1,     # \"car\"\n",
        "  11: 2,     # \"bicycle\"\n",
        "  13: 5,     # \"bus\" mapped to \"other-vehicle\" --------------------------mapped\n",
        "  15: 3,     # \"motorcycle\"\n",
        "  16: 5,     # \"on-rails\" mapped to \"other-vehicle\" ---------------------mapped\n",
        "  18: 4,     # \"truck\"\n",
        "  20: 5,     # \"other-vehicle\"\n",
        "  30: 6,     # \"person\"\n",
        "  31: 7,     # \"bicyclist\"\n",
        "  32: 8,     # \"motorcyclist\"\n",
        "  40: 9,     # \"road\"\n",
        "  44: 10,    # \"parking\"\n",
        "  48: 11,    # \"sidewalk\"\n",
        "  49: 12,    # \"other-ground\"\n",
        "  50: 13,    # \"building\"\n",
        "  51: 14,    # \"fence\"\n",
        "  52: 0,     # \"other-structure\" mapped to \"unlabeled\" ------------------mapped\n",
        "  60: 9,     # \"lane-marking\" to \"road\" ---------------------------------mapped\n",
        "  70: 15,    # \"vegetation\"\n",
        "  71: 16,    # \"trunk\"\n",
        "  72: 17,    # \"terrain\"\n",
        "  80: 18,    # \"pole\"\n",
        "  81: 19,    # \"traffic-sign\"\n",
        "  99: 0,     # \"other-object\" to \"unlabeled\" ----------------------------mapped\n",
        "  252: 1,    # \"moving-car\" to \"car\" ------------------------------------mapped\n",
        "  253: 7,    # \"moving-bicyclist\" to \"bicyclist\" ------------------------mapped\n",
        "  254: 6,    # \"moving-person\" to \"person\" ------------------------------mapped\n",
        "  255: 8,    # \"moving-motorcyclist\" to \"motorcyclist\" ------------------mapped\n",
        "  256: 5,    # \"moving-on-rails\" mapped to \"other-vehicle\" --------------mapped\n",
        "  257: 5,    # \"moving-bus\" mapped to \"other-vehicle\" -------------------mapped\n",
        "  258: 4,    # \"moving-truck\" to \"truck\" --------------------------------mapped\n",
        "  259: 5,    # \"moving-other\"-vehicle to \"other-vehicle\" ----------------mapped\n",
        "}\n",
        "\n",
        "class_weights = { # as a ratio with the total number of points\n",
        "  0: 0.018889854628292943,\n",
        "  1: 0.0002937197336781505,\n",
        "  10: 0.040818519255974316,\n",
        "  11: 0.00016609538710764618,\n",
        "  13: 2.7879693665067774e-05,\n",
        "  15: 0.00039838616015114444,\n",
        "  16: 0.0,\n",
        "  18: 0.0020633612104619787,\n",
        "  20: 0.0016218197275284021,\n",
        "  30: 0.00017698551338515307,\n",
        "  31: 1.1065903904919655e-08,\n",
        "  32: 5.532951952459828e-09,\n",
        "  40: 0.1987493871255525,\n",
        "  44: 0.014717169549888214,\n",
        "  48: 0.14392298360372,\n",
        "  49: 0.0039048553037472045,\n",
        "  50: 0.1326861944777486,\n",
        "  51: 0.0723592229456223,\n",
        "  52: 0.002395131480328884,\n",
        "  60: 4.7084144280367186e-05,\n",
        "  70: 0.26681502148037506,\n",
        "  71: 0.006035012012626033,\n",
        "  72: 0.07814222006271769,\n",
        "  80: 0.002855498193863172,\n",
        "  81: 0.0006155958086189918,\n",
        "  99: 0.009923127583046915,\n",
        "  252: 0.001789309418528068,\n",
        "  253: 0.00012709999297008662,\n",
        "  254: 0.00016059776092534436,\n",
        "  255: 3.745553104802113e-05,\n",
        "  256: 0.0,\n",
        "  257: 0.00011351574470342043,\n",
        "  258: 0.00010157861367183268,\n",
        "  259: 4.3840131989471124e-05,\n",
        "}\n",
        "\n",
        "total_weights = 0\n",
        "mapped_class_weights = []\n",
        "for cls in range(20):\n",
        "  weight = 0\n",
        "  for key in learning_map:\n",
        "    if learning_map[key]==cls:\n",
        "      weight+=class_weights[key]\n",
        "      total_weights+=class_weights[key]\n",
        "  mapped_class_weights.append(1/weight)\n",
        "\n",
        "weights = np.asarray(mapped_class_weights)\n",
        "weights = torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "class_encoding={ \n",
        "  \"unlabeled\" : 0,\n",
        "  \"car\" :1,\n",
        "  \"bicycle\" : 2,\n",
        "  \"motorcycle\" : 3,\n",
        "  \"truck\" : 4,\n",
        "  \"other-vehicle\":5,\n",
        "  \"person\":6,\n",
        "  \"bicyclist\":7,\n",
        "  \"motorcyclist\":8,\n",
        "  \"road\":9,\n",
        "  \"parking\":10,\n",
        "  \"sidewalk\":11,\n",
        "  \"other-ground\":12,\n",
        "  \"building\":13,\n",
        "  \"fence\":14,\n",
        "  \"vegetation\":15,\n",
        "  \"trunk\":16,\n",
        "  \"terrain\":17,\n",
        "  \"pole\":18,\n",
        "  \"traffic-sign\":19,\n",
        "}\n",
        "\n",
        "def map_labels(mask, lr_map = learning_map):\n",
        "  new_mask=np.zeros(mask.shape)\n",
        "  for i in range(mask.shape[0]):\n",
        "    for j in range(mask.shape[1]):\n",
        "      new_mask[i,j]=lr_map[int(mask[i,j])]\n",
        "  return new_mask\n",
        "\n",
        "\n",
        "\n",
        "class RangeKitti(data.Dataset):  \n",
        "  def __init__(self, root_dir, mode):\n",
        "\n",
        "    # SPLIT IN TRAIN / WITH / WITHOUT LABELS\n",
        "    train_with_labels = ['00', '01', '02', '03']\n",
        "    train_without_labels = ['04', '05', '06', '07']\n",
        "    test =  ['08', '09']\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "\n",
        "    if mode=='train_with_labels':\n",
        "      folders = train_with_labels\n",
        "\n",
        "    elif mode=='train_without_labels':\n",
        "      folders = train_without_labels\n",
        "\n",
        "    elif mode=='test':\n",
        "      folders = test\n",
        "\n",
        "    else :\n",
        "      print(\"unkown mode\")\n",
        "\n",
        "    self.files = []\n",
        "    for folder in folders:\n",
        "      files = get_files(self.root_dir+'/'+folder+'/range')\n",
        "      files = [self.root_dir+'/'+folder+'/range/'+file for file in files]\n",
        "      self.files.extend(files)\n",
        "    #self.files = self.files[:20]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    path = self.files[index]\n",
        "    proj = np.fromfile(path,dtype=np.int32).reshape(64,1024,6)\n",
        "    proj = proj.astype(np.float32)/1000\n",
        "\n",
        "    mask = map_labels(proj[:,:,5])\n",
        "\n",
        "    proj = torch.from_numpy(proj[:,:,0:5])\n",
        "    proj = torch.transpose(proj, 0,2)\n",
        "    proj = torch.transpose(proj, 1,2)\n",
        "    return proj, torch.from_numpy(mask).long()\n"
      ],
      "metadata": {
        "id": "oVaeomRULRKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "2qLs5tmILMkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_7baCFLb0-B"
      },
      "outputs": [],
      "source": [
        "class Metric(object):\n",
        "    \"\"\"Base class for all metrics.\n",
        "    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n",
        "    \"\"\"\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def add(self):\n",
        "        pass\n",
        "\n",
        "    def value(self):\n",
        "        pass\n",
        "\n",
        "class ConfusionMatrix(Metric):\n",
        "    \"\"\"Constructs a confusion matrix for a multi-class classification problems.\n",
        "    Does not support multi-label, multi-class problems.\n",
        "    Keyword arguments:\n",
        "    - num_classes (int): number of classes in the classification problem.\n",
        "    - normalized (boolean, optional): Determines whether or not the confusion\n",
        "    matrix is normalized or not. Default: False.\n",
        "    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, normalized=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conf = np.ndarray((num_classes, num_classes), dtype=np.int32)\n",
        "        self.normalized = normalized\n",
        "        self.num_classes = num_classes\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.conf.fill(0)\n",
        "\n",
        "    def add(self, predicted, target):\n",
        "        \"\"\"Computes the confusion matrix\n",
        "        The shape of the confusion matrix is K x K, where K is the number\n",
        "        of classes.\n",
        "        Keyword arguments:\n",
        "        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n",
        "        predicted scores obtained from the model for N examples and K classes,\n",
        "        or an N-tensor/array of integer values between 0 and K-1.\n",
        "        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n",
        "        ground-truth classes for N examples and K classes, or an N-tensor/array\n",
        "        of integer values between 0 and K-1.\n",
        "        \"\"\"\n",
        "        # If target and/or predicted are tensors, convert them to numpy arrays\n",
        "        if torch.is_tensor(predicted):\n",
        "            predicted = predicted.cpu().numpy()\n",
        "        if torch.is_tensor(target):\n",
        "            target = target.cpu().numpy()\n",
        "\n",
        "        assert predicted.shape[0] == target.shape[0], \\\n",
        "            'number of targets and predicted outputs do not match'\n",
        "\n",
        "        if np.ndim(predicted) != 1:\n",
        "            assert predicted.shape[1] == self.num_classes, \\\n",
        "                'number of predictions does not match size of confusion matrix'\n",
        "            predicted = np.argmax(predicted, 1)\n",
        "        else:\n",
        "            assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n",
        "                'predicted values are not between 0 and k-1'\n",
        "\n",
        "        if np.ndim(target) != 1:\n",
        "            assert target.shape[1] == self.num_classes, \\\n",
        "                'Onehot target does not match size of confusion matrix'\n",
        "            assert (target >= 0).all() and (target <= 1).all(), \\\n",
        "                'in one-hot encoding, target values should be 0 or 1'\n",
        "            assert (target.sum(1) == 1).all(), \\\n",
        "                'multi-label setting is not supported'\n",
        "            target = np.argmax(target, 1)\n",
        "        else:\n",
        "            assert (target.max() < self.num_classes) and (target.min() >= 0), \\\n",
        "                'target values are not between 0 and k-1'\n",
        "\n",
        "        # hack for bincounting 2 arrays together\n",
        "        x = predicted + self.num_classes * target\n",
        "        bincount_2d = np.bincount(\n",
        "            x.astype(np.int32), minlength=self.num_classes**2)\n",
        "        assert bincount_2d.size == self.num_classes**2\n",
        "        conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n",
        "\n",
        "        self.conf += conf\n",
        "\n",
        "    def value(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Confustion matrix of K rows and K columns, where rows corresponds\n",
        "            to ground-truth targets and columns corresponds to predicted\n",
        "            targets.\n",
        "        \"\"\"\n",
        "        if self.normalized:\n",
        "            conf = self.conf.astype(np.float32)\n",
        "            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n",
        "        else:\n",
        "            return self.conf\n",
        "\n",
        "\n",
        "\n",
        "class IoU(Metric):\n",
        "    \"\"\"Computes the intersection over union (IoU) per class and corresponding\n",
        "    mean (mIoU).\n",
        "    Intersection over union (IoU) is a common evaluation metric for semantic\n",
        "    segmentation. The predictions are first accumulated in a confusion matrix\n",
        "    and the IoU is computed from it as follows:\n",
        "        IoU = true_positive / (true_positive + false_positive + false_negative).\n",
        "    Keyword arguments:\n",
        "    - num_classes (int): number of classes in the classification problem\n",
        "    - normalized (boolean, optional): Determines whether or not the confusion\n",
        "    matrix is normalized or not. Default: False.\n",
        "    - ignore_index (int or iterable, optional): Index of the classes to ignore\n",
        "    when computing the IoU. Can be an int, or any iterable of ints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, normalized=False, ignore_index=None):\n",
        "        super().__init__()\n",
        "        self.conf_metric = ConfusionMatrix(num_classes, normalized)\n",
        "\n",
        "        if ignore_index is None:\n",
        "            self.ignore_index = None\n",
        "        elif isinstance(ignore_index, int):\n",
        "            self.ignore_index = (ignore_index,)\n",
        "        else:\n",
        "            try:\n",
        "                self.ignore_index = tuple(ignore_index)\n",
        "            except TypeError:\n",
        "                raise ValueError(\"'ignore_index' must be an int or iterable\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.conf_metric.reset()\n",
        "\n",
        "    def add(self, predicted, target):\n",
        "        \"\"\"Adds the predicted and target pair to the IoU metric.\n",
        "        Keyword arguments:\n",
        "        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n",
        "        predicted scores obtained from the model for N examples and K classes,\n",
        "        or (N, H, W) tensor of integer values between 0 and K-1.\n",
        "        - target (Tensor): Can be a (N, K, H, W) tensor of\n",
        "        target scores for N examples and K classes, or (N, H, W) tensor of\n",
        "        integer values between 0 and K-1.\n",
        "        \"\"\"\n",
        "        # Dimensions check\n",
        "        assert predicted.size(0) == target.size(0), \\\n",
        "            'number of targets and predicted outputs do not match'\n",
        "        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n",
        "            \"predictions must be of dimension (N, H, W) or (N, K, H, W)\"\n",
        "        assert target.dim() == 3 or target.dim() == 4, \\\n",
        "            \"targets must be of dimension (N, H, W) or (N, K, H, W)\"\n",
        "\n",
        "        # If the tensor is in categorical format convert it to integer format\n",
        "        if predicted.dim() == 4:\n",
        "            _, predicted = predicted.max(1)\n",
        "        if target.dim() == 4:\n",
        "            _, target = target.max(1)\n",
        "\n",
        "        self.conf_metric.add(predicted.view(-1), target.view(-1))\n",
        "\n",
        "    def value(self):\n",
        "        \"\"\"Computes the IoU and mean IoU.\n",
        "        The mean computation ignores NaN elements of the IoU array.\n",
        "        Returns:\n",
        "            Tuple: (IoU, mIoU). The first output is the per class IoU,\n",
        "            for K classes it's numpy.ndarray with K elements. The second output,\n",
        "            is the mean IoU.\n",
        "        \"\"\"\n",
        "        conf_matrix = self.conf_metric.value()\n",
        "        if self.ignore_index is not None:\n",
        "            for index in self.ignore_index:\n",
        "                conf_matrix[:, self.ignore_index] = 0\n",
        "                conf_matrix[self.ignore_index, :] = 0\n",
        "        true_positive = np.diag(conf_matrix)\n",
        "        false_positive = np.sum(conf_matrix, 0) - true_positive\n",
        "        false_negative = np.sum(conf_matrix, 1) - true_positive\n",
        "\n",
        "        # Just in case we get a division by 0, ignore/hide the error\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            iou = true_positive / (true_positive + false_positive + false_negative)\n",
        "\n",
        "        return iou, np.nanmean(iou)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClassMix loss"
      ],
      "metadata": {
        "id": "fiuCQOvTV0mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_mix_loss(labeled_outputs, labels, pseudo_labeled_outputs, pseudo_labels, lambda_pseudo=0.5):\n",
        "  loss = nn.CrossEntropyLoss(weight=weights.cuda().float())\n",
        "  return loss(labeled_outputs, labels) + lambda_pseudo * loss(pseudo_labeled_outputs, pseudo_labels)"
      ],
      "metadata": {
        "id": "ry7zsbffV5Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "D0vifcUBLfYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader_with_labels, train_loader_without_labels, optim, criterion, metric, iteration_loss=False):\n",
        "    '''\n",
        "    Training script: it allows the training of one epoch of the DNN.\n",
        "    input:\n",
        "      model: the model you want to train\n",
        "      train_loader_with/without_labels: the dataloaders (the FIFO of data)\n",
        "      which will be used with / without labels\n",
        "      optim: the optimizer you use[13 19  3 14 15 11 12]\n",
        "      criterion: the criterion you want to optimize\n",
        "      metric: other criteria\n",
        "      iteration_loss : boolean that allow you to print the loss\n",
        "    output:\n",
        "      epoch_loss: the loss of the full epoch\n",
        "      metric.value(): the value of the other criteria\n",
        "    '''  \n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "\n",
        "    # size\n",
        "    train_loader_with_labels = iter(train_loader_with_labels)\n",
        "    train_loader_without_labels = iter(train_loader_without_labels)\n",
        "\n",
        "    if(len(train_loader_with_labels) > len(train_loader_without_labels)):\n",
        "      train_loader_without_labels = cycle(train_loader_without_labels)\n",
        "    if(len(train_loader_without_labels) > len(train_loader_with_labels)):\n",
        "      train_loader_with_labels = cycle(train_loader_with_labels)\n",
        "\n",
        "    for step, (batch_data_with_labels, batch_data_without_labels) in enumerate(zip(train_loader_with_labels, train_loader_without_labels)):\n",
        "        # Get the inputs and labels\n",
        "        input_with_labels = batch_data_with_labels[0].cuda()\n",
        "        labels = batch_data_with_labels[1].cuda()\n",
        "        input_without_labels = batch_data_without_labels[0].cuda()\n",
        "\n",
        "        optim.zero_grad()\n",
        "\n",
        "\n",
        "        # CLASSMIX PART (pseudo labels generation and mixing)\n",
        "        # reshape\n",
        "        input_without_labels_1 = input_without_labels[0].reshape(1, *tuple(input_without_labels[0].size())) \n",
        "        input_without_labels_2 = input_without_labels[1].reshape(1, *tuple(input_without_labels[1].size()))\n",
        "        # forward\n",
        "        pred_labels_1 = model(input_without_labels_1)\n",
        "        pred_labels_2 = model(input_without_labels_2)\n",
        "        pseudo_labels_1 = torch.max(pred_labels_1, dim=1)[1] # -> argmax\n",
        "        pseudo_labels_2 = torch.max(pred_labels_2, dim=1)[1] # -> argmax\n",
        "        # mixing\n",
        "        classes_in_pseudo_labels_1 = torch.unique(pseudo_labels_1).tolist()\n",
        "        sub_set_classes = np.random.choice(classes_in_pseudo_labels_1, \\\n",
        "                                           size=len(classes_in_pseudo_labels_1) // 2, replace=False) # sub set of detected classes of size number of classes // 2\n",
        "        mask = torch.zeros_like(pseudo_labels_1)\n",
        "        for c in sub_set_classes:\n",
        "          mask[torch.where(pseudo_labels_1 == c)] = 1 # to keep classes in sub_set_classes\n",
        "\n",
        "        mixed_input = mask * input_without_labels_1 + (1 - mask) * input_without_labels_2 # final mixed image\n",
        "        mixed_pseudo_labels = mask * pseudo_labels_1 + (1 - mask) * pseudo_labels_2 # corresponding pseudo_labels\n",
        "      \n",
        "\n",
        "        # Forward (labeled / pseudo labeled)\n",
        "        output_labeled = model(input_with_labels)\n",
        "        output_pseudo_labeled = model(mixed_input)\n",
        "        # print('input_with_labels : ', input_with_labels.size())\n",
        "        # print('mixed_input : ', mixed_input.size())\n",
        "        # print('output_labeled : ', output_labeled.size())\n",
        "        # print('labels : ', labels.size())\n",
        "        # print('output_pseudo_labeled : ', output_pseudo_labeled.size())\n",
        "        # print('mixed_pseudo_labels : ', mixed_pseudo_labels.size())\n",
        "\n",
        "        # Loss computation (combines labeled / pseudo labeled) \n",
        "        loss = criterion(output_labeled, labels, output_pseudo_labeled, mixed_pseudo_labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of the evaluation metric\n",
        "        output_concat = torch.cat((output_labeled, output_pseudo_labeled), dim=0) # concat labeled / pseudo labeled output\n",
        "        labels_concat = torch.cat((labels, mixed_pseudo_labels), dim=0) # concat labels / pseudo labels\n",
        "        metric.add(output_concat.detach(), labels_concat.detach())\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "\n",
        "    return epoch_loss / len(train_loader_with_labels), metric.value()\n",
        "\n",
        "def test(model, test_loader, criterion, metric, iteration_loss=False):\n",
        "    '''\n",
        "    Validation script: it allows the validationof the DNN.\n",
        "    input:\n",
        "      model: the DNN you want to train\n",
        "      test_loader: the dataloader (the FIFO of data)\n",
        "      criterion: the criterion you hav optimized\n",
        "      metric: other criteria\n",
        "      iteration_loss : boolean that allow you to print the loss\n",
        "    output:\n",
        "      epoch_loss: the loss of the full epoch\n",
        "      metric.value(): the value of the other criteria\n",
        "    '''  \n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    \n",
        "    for step, batch_data in enumerate(test_loader):\n",
        "        # Get the inputs and labels\n",
        "        inputs = batch_data[0].cuda()\n",
        "        labels = batch_data[1].cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward propagation\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of evaluation the metric\n",
        "        metric.add(outputs.detach(), labels.detach())\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "    for classe, res in zip(class_encoding, metric.value()[0]):\n",
        "        print(f\"[{classe}] : {res}\")\n",
        "    return epoch_loss / len(test_loader), metric.value()"
      ],
      "metadata": {
        "id": "x83q3O12LPQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backbone"
      ],
      "metadata": {
        "id": "PiPmSfeELpeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        if self.bilinear:\n",
        "            x1 = F.interpolate(x1, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            x1 = self.up(x1)\n",
        "\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # for padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, classes=20):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(5, 32)\n",
        "\n",
        "        self.down1 = down(32, 64)\n",
        "        self.down2 = down(64, 128)\n",
        "        self.down3 = down(128, 256)\n",
        "        self.down4 = down(256, 256)\n",
        "\n",
        "        self.up4 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up2 = up(128, 32)\n",
        "        self.up1 = up(64, 32)\n",
        "\n",
        "        self.outconv = outconv(32, classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # please complete\n",
        "        x0 = x\n",
        "        \n",
        "        x0 = self.inc(x0)\n",
        "\n",
        "        d1 = self.down1(x0)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "\n",
        "        u4 = self.up4(d4, d3)\n",
        "        u3 = self.up3(u4, d2)\n",
        "        u2 = self.up2(u3, d1)\n",
        "        u1 = self.up1(u2, x0)\n",
        "\n",
        "        x = self.outconv(u1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mtnSfPWyLrdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cV4CU_z0L0zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL\n",
        "model = UNet(classes=20)\n",
        "\n",
        "# DATA\n",
        "data_path = 'range_dataset' # YOUSSEF\n",
        "# data_path = 'drive/My Drive/Semantic_segmentation_IA321/data/sequences' # MAXIME\n",
        "\n",
        "\n",
        "train_set_with_labels = RangeKitti(data_path, mode='train_with_labels')\n",
        "train_set_without_labels = RangeKitti(data_path, mode='train_without_labels')\n",
        "val_set = RangeKitti(data_path, mode='test')\n",
        "\n",
        "\n",
        "\n",
        "train_with_labels_loader = data.DataLoader(\n",
        "        train_set_with_labels,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "# BATCH SIZE MUST BE 2 FOR NO LABELS PART (HERE WE MIX ONLY 2 IMAGES)\n",
        "train_without_labels_loader = data.DataLoader(\n",
        "        train_set_without_labels,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "val_loader = data.DataLoader(\n",
        "        val_set,\n",
        "        batch_size=2,\n",
        "        shuffle=True,\n",
        "        num_workers=2)\n",
        "\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "# here are the training parameters\n",
        "learning_rate =1e-3\n",
        "weight_decay=2e-4\n",
        "lr_decay_epochs=20\n",
        "lr_decay=0.1\n",
        "nb_epochs=50\n",
        "\n",
        "\n",
        "# ClassMix loss \n",
        "train_criterion = class_mix_loss\n",
        "test_criterion = nn.CrossEntropyLoss(weight=weights.cuda().float())\n",
        "\n",
        "# We build the optimizer\n",
        "optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate decay scheduler\n",
        "lr_updater = lr_scheduler.StepLR(optimizer, lr_decay_epochs,\n",
        "                                     lr_decay)\n",
        "\n",
        "# Evaluation metric\n",
        "ignore_index=[]\n",
        "#ignore_index0 = list(class_encoding).index('unlabeled')\n",
        "ignore_index.append(0)\n",
        "metric = IoU(20, ignore_index=ignore_index)\n",
        "\n",
        "# Start Training\n",
        "best_miou = 0\n",
        "train_loss_history_1 = []\n",
        "val_loss_history_1 = []\n",
        "train_miou_history_1 = []\n",
        "val_miou_history_1 = []\n",
        "\n",
        "for epoch in range( nb_epochs):\n",
        "  print(\">>>> [Epoch: {0:d}] Training\".format(epoch))\n",
        "  epoch_loss, (iou, miou) = train(model, train_with_labels_loader, train_without_labels_loader, optimizer, train_criterion, metric) \n",
        "  lr_updater.step()\n",
        "  print(\">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}\".format(epoch, epoch_loss, miou))\n",
        "  train_miou=miou\n",
        "  train_loss=epoch_loss\n",
        "\n",
        "  if (epoch + 1) % 5 == 0 or epoch + 1 == nb_epochs:\n",
        "    print(\">>>> [Epoch: {0:d}] Validation\".format(epoch))\n",
        "    loss, (iou, miou) = test(model, val_loader, test_criterion, metric)\n",
        "    print(\">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}\".format(epoch, loss, miou))\n",
        "\n",
        "    train_loss_history_1.append(train_loss)\n",
        "    val_loss_history_1.append(loss)\n",
        "    train_miou_history_1.append(train_miou)\n",
        "    val_miou_history_1.append(miou)\n",
        "    # Print per class IoU on last epoch or if best iou\n",
        "    if epoch + 1 == nb_epochs or miou > best_miou:\n",
        "      for key, class_iou in zip(class_encoding.keys(), iou):\n",
        "        print(\"{0}: {1:.4f}\".format(key, class_iou))\n",
        "        # Save the model if it's the best thus far\n",
        "        if miou > best_miou:\n",
        "          print(\"\\nBest model thus far. Saving...\\n\")\n",
        "          best_miou = miou\n",
        "          torch.save(model.state_dict(), \"Unet_epoch{}.pt\".format(epoch+1))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"Unet_epoch{}_final.pt\".format(nb_epochs))\n",
        "\n",
        "print('train_loss_history_1', train_loss_history_1)\n",
        "print('val_loss_history_1',val_loss_history_1)\n",
        "print('train_miou_history_1',train_miou_history_1)\n",
        "print('val_miou_history_1',val_miou_history_1)"
      ],
      "metadata": {
        "id": "CmQaylHhL1uE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "37ee9cc3-8335-485c-ea37-d0abc47e165b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> [Epoch: 0] Training\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0fe6b9ee4686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>> [Epoch: {0:d}] Training\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_with_labels_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_without_labels_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0mlr_updater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-b0392dc9d1d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader_with_labels, train_loader_without_labels, optim, criterion, metric, iteration_loss)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}